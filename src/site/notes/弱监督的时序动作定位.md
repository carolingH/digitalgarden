---
{"dg-publish":true,"permalink":"/弱监督的时序动作定位/"}
---


# #SF-Net:Single-Frame Supervision for Temporal Action Localization

code: [https://github.com/Flowerfan/SF-Net](https://github.com/Flowerfan/SF-Net)

University of Technology Sydney, Australia

source: ECCV2020

![](file://D:\Code\Master\emotion-video\papers-24\SF.png?lastModify=1725270134)

上图中，已经标注，全监督，弱监督，单帧监督的标签形式。
( 1 )据我们所知，这是第一个使用单帧监督来解决动作时间边界定位问题的工作。我们发现，与强监督标注相比，单帧标注显著节省了标注时间。

( 2 )我们发现单帧监督可以提供关于背景的强线索。因此，从未标注的帧中，我们提出了两种新颖的方法，分别挖掘可能的背景帧和动作帧。这些可能的背景和动作时间戳被进一步用作训练的伪真值。

( 3 )我们在三个基准数据集上进行了大量的实验，在片段定位和单帧定位任务上的性能都得到了很大的提升。

**两个时间定位任务：**（1）**Segment localization.** 我们用其动作类预测来检测每个动作实例的开始时间和结束时间。 （2）**Single frame localization.** 我们输出每个检测到的动作实例的时间戳及其动作类预测
![](file://D:\Code\Master\emotion-video\papers-24\SF-net.png?lastModify=1725276275)

x（N x**T** x D） 分**N个batch**

c (N x T x C+1) x C为类别 L帧

C pooling 后（k s） Nx1xC+1 video

A (N xT )(actionness score)

k为真实帧 K<<NT pl (Rx Nc+1)

K 是标注帧的数量。
yi 是第 i 个标注帧的实际标签。pi 是第 i个标注帧预测的概率。
$$
L_{frame}^l=-\frac1K\sum_i^K\mathrm{y}_i\mathrm{logp}_i^l
$$

单帧标注：（Action frame mining）

我们将每个已标注的动作帧作为每个动作实例的锚框，设置扩展半径r来限制t时刻到锚框的最大扩展距离。然后将过去从t - 1帧展开，未来从t + 1帧展开，如果当前扩展框架与锚框架具有相同的预测标签，并且在yi类的分类得分高于锚框架乘以一个预定义的值ξ的得分，标注为yi,否则结束扩展。
```python
def get_rand_frame_labels(self):

        classlist = self.get_classlist()

        labels = []

        for i, vid_seg in enumerate(self.segments):

            max_len = len(self.features[i])

            fps = self.get_fps(i)

            assert len(vid_seg) == len(self.gtlabels[i])

            #  frame_label = np.array([-1] * len(self.features[i]))

            frame_label = [[] for _ in range(max_len)]

            if len(vid_seg) < 1:

                labels += [frame_label]

            else:

                for seg, l in zip(vid_seg, self.gtlabels[i]):

                    intl = utils.strlist2indlist([l], classlist)[0]

                    start, end = np.array(seg) * fps / self.stride

                    start = max(0, int(np.ceil(start)))

                    end = min(max_len, int(end))

                    if end <= start:

                        continue

                    mid = np.random.choice(range(start, end), 1)[0]

                    if intl not in frame_label[mid]:

                        frame_label[mid].append(intl)

                labels += [frame_label]

        return labels
```

Background frame mining

单帧标注下，没有背景标签，从未标注的帧中定位背景帧。（选择​个background）
$$
\mathcal{L}_{frame}^b=-\frac1{nK}\sum\log\text{p}^b,
$$
计算出
$$
\mathcal{L}_{frame}=\mathcal{L}_{frame}^l+\frac{1}{N_c}\mathcal{L}_{frame}^b$$

$$\mathcal{L}_{actionness}=-\frac{1}{K}\sum\log\sigma(A^{l})-\frac{1}{\eta K}\sum\log(1-\sigma(A^{b})),$$
BCEloss (Actionness module A （Nx T）)
A:​ ​Al Ab  (labeled frame unlabled frame) $$A^l (有标注)A^b(无标签)$$
视频loss ​
$$\mathcal{L}_{video}=-\frac1N\sum_{i=1}^N\sum_{j=1}^{N_c}q_i(j)\log\frac{\exp(r_i(j))}{\sum_{N_c+1}\exp(r_i(k))}$$

​最终的loss计算
$$
l=l_{frame}^l+αl_actionness+βl_video$$