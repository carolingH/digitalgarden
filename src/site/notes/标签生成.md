---
{"dg-publish":true,"permalink":"/标签生成/"}
---

SF-net
--
[[弱监督的时序动作定位\|弱监督的时序动作定位]]()

#伪标签生成





## Temporal Sentiment Localization: Listen and Look in Untrimmed Videos
--
[[时序情绪定位\|时序情绪定位]]
[GitHub - nku-zhichengzhang/TSL300: [ACM MM 2022] This is the official implementation of "Temporal Sentiment Localization: Listen and Look in Untrimmed Videos"](https://github.com/nku-zhichengzhang/TSL300)
文中：
[[]]
![Pasted image 20240902173437.png](/img/user/picture/Pasted%20image%2020240902173437.png)

''''
```python
           temp_anno = np.zeros([vid_num_seg, self.num_classes], dtype=np.float32)

            # t_factor = self.feature_fps / (self.fps_dict[vid_name] * 16)

            t_factor = 1/16

            temp_df = self.point_anno[self.point_anno["video_id"] == vid_name][['point', 'class_index']]

            '''

            point class_index 两部分

            '''
            for key in temp_df['point'].keys():

                point = temp_df['point'][key]

                class_idx = temp_df['class_index'][key]

                #

                # label smooth

                temp_anno[int(point * t_factor)] = 0.1

                temp_anno[int(point * t_factor)][class_idx] = 0.9

                # MLL

                label[class_idx] = 1

                # smoothed MLL

                label_distribution[class_idx] += 1

            # hard label

            # label_distribution = label

            # number distribution

            label_distribution = label_distribution/label_distribution.sum()

            # label smooth

            # if label_distribution[0] == label_distribution[1]:

            #     label_distribution[0] = label_distribution[1]=0.5

            # else:

            #     ind = np.argmax(label_distribution)

            #     label_distribution = np.ones_like(label_distribution)*0.1

            #     label_distribution[ind] = 0.9

            point_label = temp_anno[sample_idx, :]
            return label, torch.from_numpy(point_label), label_distribution
```
 
#SF-TAL
single-frame supervise
伪标签---
Neighbor-Guided Pseudo-Label Generation and Refinement for Single-Frame Supervised Temporal Action Localization
--
[Neighbor-Guided Pseudo-Label Generation and Refinement for Single-Frame Supervised Temporal Action Localization | IEEE Journals & Magazine | IEEE Xplore]
[(https://ieeexplore.ieee.org/abstract/document/10478311)]
{ #e7c1bf}


temporal neighbor-guided soft pseudo-lable generation(TNPG)
semantic neighbor-guided pseudo-lable refinement( SNPR)
针对SF - TAL任务，提出了一种邻域引导的软伪标签生成和精化( Neighbor-guided soft Pseudolabel Generation and Refinement，NGPR )方法，以前的方法基于固定的阈值相比，通过关注每个帧的时间邻居来为所有帧构建伪标注，并将标记和未标记帧进行关联，提高了数据利用率。

针对SF - TAL，提出了一种时间近邻引导的伪标签生成方法，该方法通过局部-全局Transformer编码器捕获帧间的时间关系，为有标签帧和无标签帧生成软伪标注，从而提高数据利用率；

针对SF - TAL，本文提出了一种语义近邻引导的伪标签精化方法，该方法通过预测每一帧的语义近邻来精化每一帧的软伪标签，以抑制伪标签噪声。
![NGPR.png](/img/user/picture/NGPR.png)

X->F(TXD)->(经历local-global self -attention)产生语义特征Fs∈R (T × D)。这些特征被分类生成T - CAS S∈ R (T × ( C + 1 ))
在local-global时，设计了factio(1XD) 上面的橙色，（输入是拼接了这一特征的 T+1 X D数据） 生成全局attentio Mg 和局部注意力 Ml 有这两得到$$M_{act}$$,计算得到Fs
snip对应actionness
Temporal Neighbor-Guided Soft Pseudo-Label Generation(计算公式P5)
对Fs进行增强,引入新的mse loss

![NGPR-topk.png](/img/user/picture/NGPR-topk.png)



Realigning Confidence with Temporal Saliency Information for Point-Level Weakly-Supervised Temporal Action Localization
--
code:https://github.com/zyxia1009/CVPR2024-TSPNet.

提出了一种新颖的提案级插件框架，以重新学习由基础定位器生成的提案的对齐置信度。为P - TAL引入即插即用的时间显著性驱动的提案学习框架( TSP-Net )
频率统计显示了在三个基准中出现在不同位置的注释点的分布。(图文中统计 类似一个正态分布)观察到最显著的框架倾向于出现在每个实例的中心区域，并且容易被人类标注，称这种先验为"时间显著性信息"。这些信息揭示了实例中心的大致位置，并提供了对齐的证据。
![TSP_Net.png](/img/user/picture/TSP_Net.png)
**中间标签的生成**
标注更注重的是显著的点，忽视边界信息首先为每个实例引入显著点(可以看成一个伪中心)来表示最显著的时间戳，该点可以作为实例中心的向导。在实际应用中，我们使用点标注Y初始化显著性点集Psal。随后，考虑到最完整提议的中心及其对应的真实实例应该与显著点接近或对齐，我们手动生成第i个proposal的硬中心标签为

$$
y_{cen,i}=\begin{cases}
1-2\left|\frac{t-s_{i}}{e_{i}-s_{i}}-0.5\right| & \text{if } \exists t\in P_{\text{sal}}, t\in[s_{i},e_{i}] \\
0 & \text{if } \forall t\in P_{\text{sal}}, t\notin[s_{i},e_{i}]
\end{cases}. \qquad$$
由于缺少非标签的信息，提出软中心标签
$$y_{cen,i}=\begin{cases}
1-2\left|\frac{t-s_{i}}{e_{i}-s_{i}}-0.5\right| & \text{if } \exists t\in P_{\text{sal}}, t\in[s_{i},e_{i}] \\
λ & \text{if } \forall t\in P_{\text{sal}}, t\notin[s_{i},e_{i}]
\end{cases}. \qquad
$$

**显著性点的更新**
对于每个显着点，首先挖掘包含该点并且场景大于更新阈值θup的提议集Sup。其次，挖掘出的提案坐标中的中心被称为伪点，包含有价值的中心信息。接下来，通过显着点和伪点之间的差异的加权和来更新点。最后，重复该操作来更新整个显着点集。为了保持有效性和稳定性，根据初始注释点 Y 更新每次 Pup迭代的显着点。更新后，我们重新生成中心标签以学习更好的场景。
![TSP-Net1.png](/img/user/picture/TSP-Net1.png)
**对其边界Adaption**
将 softmax 函数应用于 sc，产生 pc。随后，融合 TSP-Net 的Sa Scen Pc生成的分数以获得第 i 个类别的提案的对齐置信度 ci ∈ (M×1 )

POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization
--

![POTLoc.png](/img/user/picture/POTLoc.png)
提出了 POTLoc，这是一个面向弱监督动作定位的伪标签导向的变换器，仅使用点级标注。POTLoc 设计用于通过自训练策略识别和跟踪连续的动作结构。基础模型首先仅使用点级监督生成动作提议。这些提议经过细化和回归以提高估计动作边界的精度，随后产生用作补充监督信号的“伪标签”。

标签问题

全监督：帧级别 frame by frame

弱监督： video-level 只有视频级标签，没有帧级标签，也就是说动作序列的起始帧和终止帧没有标注
弱监督：single-frame 时间戳


##PivoTAL: Prior-Driven Supervision for Weakly-Supervised Temporal Action Localization
--
source: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

![](file://D:\Code\Master\emotion-video\papers-24\PivoTAL-a.png?lastModify=1725345205)

f:g -> ​ (​ (d)),​ (Txd)(T 片段数)是video-level 特征，

classifier ​ ->**q**(T x C+1) **CAS score** actionness scores **a** (T*2)(foreground and background)

**hadamard product** ​ ​​$$\hat{y}_{fg}=1/K\sum_{k=1}^{\mathbf{n}}\mathrm{topK}(\mathbf{a}_{fg}\odot\mathbf{q})​​$$对应位置乘
$$\mathcal{L}_{fg}=-\sum_{c=1}^{C+1}\mathbf{y}(c)\log\mathbf{\hat{y}}_{fg}(c).$$
​ ​ 同样
$$\mathcal{L}_{base}=\mathcal{L}_{fg}+\mathcal{L}_{bg}$$
​

Snippet Generator (SG) q->A({cj, sj, ej})

下图高斯先验
$$\mathbf{G}_{i}=\exp\Big(-\frac{\beta(j/T-\mu_i)^2}{\sigma_i^2}\Big)_{j=1}^T,\\\mathbf{a}_{gauss}=\{\mathbf{G}_i(i)\}_{i=1}^T,\text{}$$

loss​
$$\mathcal{L}_{con}=\sum_i(\mathbf{a}_{fg}(i)-\mathbf{a}_{gauss}(i))^2$$The Base WTAL Head ​
$$\mathcal{L}_{base}=\mathcal{L}_{fg}+\mathcal{L}_{bg}+\mathcal{L}_{con}.$$

![](file://D:\Code\Master\emotion-video\papers-24\PivoTAl-bc.png?lastModify=1725345205)

Scene Prior
$$\tilde{\mathbf{y}}_{bg}=\alpha\mathbf{y}_{bg}+(1-\alpha)\mathbf{y}_{fg},
\\\mathcal{L}_{bg}=-\sum_{c=1}^{C+1}\tilde{\mathbf{y}}_{bg}(c)\log\mathbf{\hat{y}}_{bg}(c)$$
​Gaussian Mixture Proposals with Pull-Push Learning Scheme to Capture Diverse Events for Weakly Supervised Temporal Video Grounding
--
code:[GitHub - sunoh-kim/pps: Pytorch implementation of the paper 'Gaussian Mixture Proposals with Pull-Push Learning Scheme to Capture Diverse Events for Weakly Supervised Temporal Video Grounding' (AAAI2024).](https://github.com/sunoh-kim/pps)

主要是提出高斯混合方法
![Guassian mixtrue.png](/img/user/picture/Guassian%20mixtrue.png)